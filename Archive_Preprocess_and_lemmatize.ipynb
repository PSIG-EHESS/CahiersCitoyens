{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba050edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly\n",
    "# pour visualization de BERTopic\n",
    "\n",
    "# !pip install bertopic\n",
    "\n",
    "# pip install SentenceTransformer\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665f6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour un ordi avec GPU :\n",
    "\n",
    "import GPUtil\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b84c850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98135ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set CUDA_PATH=C:\\Users\\ehess\\anaconda3\\Lib\\site-packages\\numba\\cuda\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83f10f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc821ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ebbc49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 86% |\n"
     ]
    }
   ],
   "source": [
    "# pour un ordi avec GPU\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    print(\"Using GPU!\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a96769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "91fa0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"contrib_from_csv.csv\", sep = \",\", dtype= str)\n",
    "\n",
    "# sample d'un petit échantillon\n",
    "df = df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9aa74bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les fin de lignes (deux types de codages)\n",
    "join_breaklines = lambda x : x.replace(\"\\x0b\",\" \") \n",
    "join_breaklinesn = lambda x : x.replace(\"\\n\",\" \") \n",
    "\n",
    "# supprimer les UNK des ce corpus (trois types de codages)\n",
    "kill_texte_illisible = lambda x : x.replace(\"texte illisible\", \"\")\n",
    "kill_illisible = lambda x : x.replace(\"illisible\", \"\")\n",
    "kill_illisible_plural = lambda x : x.replace(\"illisibles\", \"\")\n",
    "kill_illisible_capslock = lambda x : x.replace(\"ILLISIBLE\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "44de22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"] = df[\"Contribution\"].apply(join_breaklines)\n",
    "df[\"clean\"] = df[\"clean\"].apply(join_breaklinesn)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_texte_illisible)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible_plural)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible_capslock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad06d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "Total time: 0.0009992122650146484\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "df[\"try\"] = df[\"clean\"].apply(remove_stopwords)\n",
    "t1 = time.time()\n",
    "print(\"Total time: {}\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "79835dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop) + [\"\\n\"]\n",
    "final_stopwords_set = set(final_stopwords_list)\n",
    "def remove_stopwords (text):\n",
    "    text = list(text)\n",
    "    print(text)\n",
    "    buff =[]\n",
    "    for word in text :\n",
    "        if word not in final_stopwords_list :\n",
    "            buff.append(word)\n",
    "    output = \" \".join(buff)\n",
    "#     text = [word for word in text if word not in final_stopwords_set]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2bae875",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.DataFrame([[\"test de les stopwords\",1],[\"de et donc\",2],[\"voir être arbre\",5]], columns = [\"tes\",\"bien\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef1a608f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tes</th>\n",
       "      <th>bien</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test de les stopwords</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de et donc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voir être arbre</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tes  bien\n",
       "0  test de les stopwords     1\n",
       "1             de et donc     2\n",
       "2        voir être arbre     5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8604d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'e', 's', 't', ' ', 'd', 'e', ' ', 'l', 'e', 's', ' ', 's', 't', 'o', 'p', 'w', 'o', 'r', 'd', 's']\n",
      "['d', 'e', ' ', 'e', 't', ' ', 'd', 'o', 'n', 'c']\n",
      "['v', 'o', 'i', 'r', ' ', 'ê', 't', 'r', 'e', ' ', 'a', 'r', 'b', 'r', 'e']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    t e s t   d e   l e s   s t p w r d s\n",
       "1                        d e   e t   d n c\n",
       "2                  v r   ê t r e   r b r e\n",
       "Name: tes, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf[\"tes\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ee28c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206601    impôts plus de justice fiscale par augmentatio...\n",
       "64289     Mardi 11 Décembre 2018 Veuve, ma pension n'aug...\n",
       "171719    Mr et Mr VERDOT Laurent 10.01.2010 Bonjour, vo...\n",
       "197115    Où est la république et quelle république ?? q...\n",
       "140264    - Démocratie et citoyenneté L’Europe au servic...\n",
       "119831    Voici mes subjections que j'aimerai voir appli...\n",
       "29604     DOLÉANCES I.S.F : insister ? Si fait I Monsieu...\n",
       "176874    transition énergétique est un éclairage public...\n",
       "111099                                          {{TABLEAU}}\n",
       "148098    Que les gilets jaunes mettent en place un gouv...\n",
       "Name: try, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"try\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "72fae9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218083    20 ans, l'âge de la retraite ?. Dommage, c'est...\n",
       "7427      AUTRES THÉMATIQUES D'autres sujets émergent au...\n",
       "121915    le 24.01.2019 - Baisse de TVA par les produits...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b2222b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sur le GPU\n",
    "\n",
    "def lemmatize(doc):\n",
    "    '''\n",
    "    Performs lemmization of input documents.\n",
    "    Args:\n",
    "    - docs: one column of a dataframe\n",
    "    Output:\n",
    "    - list (nb of contributions) of list (one cell,(size 2: with position input, lemmatized input)) of list of strings\n",
    "    '''\n",
    "\n",
    "    spacy.require_gpu()\n",
    "    nlp = spacy.load(\"fr_core_news_md\", disable = [\"parser\", \"ner\"])\n",
    "    nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "    \n",
    "    lemma_text_list = []  \n",
    "    lemma_pos_list = []\n",
    "    t0 = time.time()\n",
    "    for doc in nlp.pipe(doc):\n",
    "#         buffer = []\n",
    "#         lemma_text_list.append([[\" \".join(x.pos_ for x in doc)], [\" \".join(x.lemma_ for x in doc)]])     \n",
    "#         buffer.append([\" \".join(x.pos_ for x in doc)], [\" \".join(x.lemma_ for x in doc)]) \n",
    "#         print(buffer)\n",
    "#        on ne join() pas à ce moment là car la list reste nécessaire pour enlever les stopwords.\n",
    "        lemma_text_list.append([x.pos_ for x in doc])\n",
    "        lemma_pos_list.append([x.lemma_ for x in doc])\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total time: {}\".format(t1-t0))\n",
    "\n",
    "    return (lemma_text_list, lemma_pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5834a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.6529309749603271\n"
     ]
    }
   ],
   "source": [
    "# GPU Total time: 130.2461621761322 (without if and with fr_core_news_md)\n",
    "lemmatized_pos, lemmatized_lemma = lemmatize(df[\"clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "03f106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text\"] = lemmatized_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1293d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_pos\"] = lemmatized_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787836b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour afficher l'ensemble des cells, à n'utiliser que pour afficher une portion seulement\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f8edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86aaec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test CPU\n",
    "\n",
    "# def lemmatize(doc, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "#     nlp = spacy.load(\"fr_core_news_md\", disable = [\"parser\", \"ner\"])\n",
    "#     nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "#     lemma_text_list = []\n",
    "#     t0 = time.time()\n",
    "#     for doc in nlp.pipe(doc):\n",
    "#         lemma_text_list.append([[\" \".join(x.pos_ for x in doc)], [\" \".join(x.lemma_ for x in doc)]])      \n",
    "#     t1 = time.time()\n",
    "#     print(\"Total time: {}\".format(t1-t0))  # ~9.759 seconds on 5000 rows\n",
    "#     return (lemma_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d73e41f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catégorie</th>\n",
       "      <th>Date de réception</th>\n",
       "      <th>Code postal</th>\n",
       "      <th>Code INSEE</th>\n",
       "      <th>Numéro d'ordre arbitraire</th>\n",
       "      <th>Type Graphie TT</th>\n",
       "      <th>Numéro de page</th>\n",
       "      <th>Numéro séquentiel</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>joined_id</th>\n",
       "      <th>clean</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218083</th>\n",
       "      <td>CC</td>\n",
       "      <td>190315</td>\n",
       "      <td>44980</td>\n",
       "      <td>44172</td>\n",
       "      <td>16802</td>\n",
       "      <td>MD</td>\n",
       "      <td>161</td>\n",
       "      <td>76</td>\n",
       "      <td>20 ans, l'âge de la retraite ?. Dommage, c'est...</td>\n",
       "      <td>CC_44980_190315_44172_MD_16802</td>\n",
       "      <td>20 ans, l'âge de la retraite ?. Dommage, c'est...</td>\n",
       "      <td>[20, an, ,, le, âge, de, le, retraite, ?, ., d...</td>\n",
       "      <td>[NUM, NOUN, PUNCT, DET, NOUN, ADP, DET, NOUN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>37110</td>\n",
       "      <td>37116</td>\n",
       "      <td>01220</td>\n",
       "      <td>D</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>AUTRES THÉMATIQUES\u000b",
       "D'autres sujets émergent au...</td>\n",
       "      <td>CC_37110_190225_37116_MD_01220</td>\n",
       "      <td>AUTRES THÉMATIQUES D'autres sujets émergent au...</td>\n",
       "      <td>[autre, thématique, de, autre, sujet, émerger,...</td>\n",
       "      <td>[ADJ, ADJ, ADP, ADJ, NOUN, VERB, ADP, NOUN, AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121915</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>30300</td>\n",
       "      <td>30117</td>\n",
       "      <td>05247</td>\n",
       "      <td>MD</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>le 24.01.2019\\n- Baisse de TVA par les produit...</td>\n",
       "      <td>CC_30300_190225_30117_MD_05247</td>\n",
       "      <td>le 24.01.2019 - Baisse de TVA par les produits...</td>\n",
       "      <td>[le, 24.01.2019, -, baisse, de, tva, par, le, ...</td>\n",
       "      <td>[DET, PROPN, PUNCT, NOUN, ADP, NOUN, ADP, DET,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Catégorie Date de réception Code postal Code INSEE  \\\n",
       "218083        CC            190315       44980      44172   \n",
       "7427          CC            190225       37110      37116   \n",
       "121915        CC            190225       30300      30117   \n",
       "\n",
       "       Numéro d'ordre arbitraire Type Graphie TT Numéro de page  \\\n",
       "218083                     16802              MD            161   \n",
       "7427                       01220               D             51   \n",
       "121915                     05247              MD             25   \n",
       "\n",
       "       Numéro séquentiel                                       Contribution  \\\n",
       "218083                76  20 ans, l'âge de la retraite ?. Dommage, c'est...   \n",
       "7427                  21  AUTRES THÉMATIQUES\n",
       "D'autres sujets émergent au...   \n",
       "121915                 6  le 24.01.2019\\n- Baisse de TVA par les produit...   \n",
       "\n",
       "                             joined_id  \\\n",
       "218083  CC_44980_190315_44172_MD_16802   \n",
       "7427    CC_37110_190225_37116_MD_01220   \n",
       "121915  CC_30300_190225_30117_MD_05247   \n",
       "\n",
       "                                                    clean  \\\n",
       "218083  20 ans, l'âge de la retraite ?. Dommage, c'est...   \n",
       "7427    AUTRES THÉMATIQUES D'autres sujets émergent au...   \n",
       "121915  le 24.01.2019 - Baisse de TVA par les produits...   \n",
       "\n",
       "                                          lemmatized_text  \\\n",
       "218083  [20, an, ,, le, âge, de, le, retraite, ?, ., d...   \n",
       "7427    [autre, thématique, de, autre, sujet, émerger,...   \n",
       "121915  [le, 24.01.2019, -, baisse, de, tva, par, le, ...   \n",
       "\n",
       "                                           lemmatized_pos  \n",
       "218083  [NUM, NOUN, PUNCT, DET, NOUN, ADP, DET, NOUN, ...  \n",
       "7427    [ADJ, ADJ, ADP, ADJ, NOUN, VERB, ADP, NOUN, AD...  \n",
       "121915  [DET, PROPN, PUNCT, NOUN, ADP, NOUN, ADP, DET,...  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatized = lemmatize(df[\"clean\"])\n",
    "# CPU Total time: 160.70105290412903 without if and with fr_core_news_md\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c2af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b49cd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', 'an', ',', 'le', 'âge', 'de', 'le', 'retraite', '?', '.', 'dommage', ',', 'ce', 'être', 'avoir', 'ce', 'âge', 'que', 'tu', 'te', 'être', 'ouvrir', 'au', 'monde', '.', 'tu', 'ne', 'avoir', 'pas', 'le', 'droit', 'avoir', 'un', 'scolarisation', '.', 'pourtant', ',', 'le', 'progrès', 'de', 'neuroscience', '.', 'montrer', 'le', 'plasticité', 'cérébral', '.', 'tu', 'en', 'être', 'le', 'preuve', 'vivant', '.', 'le', 'motivation', 'sans', 'faille', 'de', 'ton', 'mère', '.', 'est', 'bien', 'récompenser', 'par', 'ton', 'progrès', '.', 'progrès', ',', 'réel', 'source', 'de', 'espoir', '.', 'dans', 'un', 'quotidien', 'trop', 'souvent', 'anxyogèn', '.', 'mon', 'fils', ',', 'quel', 'bataille', 'pour', 'te', 'aider', 'à', 'te', 'épanouir', '.', 'et', 'te', 'permettre', 'de', 'rejoindre', 'le', 'communauté', 'humain', '.', 'autisme', ',', 'en', '2019', ',', 'en', 'France', ',']\n"
     ]
    }
   ],
   "source": [
    "doc = df.lemmatized_text.tolist()\n",
    "def remove_stopwords(text):\n",
    "    stopwords = final_stopwords_list\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "test = remove_stopwords(doc)\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b357ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop) + [\"\\n\"]\n",
    "final_stopwords_set = set(final_stopwords_list)\n",
    "def remove_stopwords (text):\n",
    "\n",
    "    text = [print(word) for word in text] #if word not in final_stopwords_set]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9829788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', 'an', ',', 'le', 'âge', 'de', 'le', 'retraite', '?', '.', 'dommage', ',', 'ce', 'être', 'avoir', 'ce', 'âge', 'que', 'tu', 'te', 'être', 'ouvrir', 'au', 'monde', '.', 'tu', 'ne', 'avoir', 'pas', 'le', 'droit', 'avoir', 'un', 'scolarisation', '.', 'pourtant', ',', 'le', 'progrès', 'de', 'neuroscience', '.', 'montrer', 'le', 'plasticité', 'cérébral', '.', 'tu', 'en', 'être', 'le', 'preuve', 'vivant', '.', 'le', 'motivation', 'sans', 'faille', 'de', 'ton', 'mère', '.', 'est', 'bien', 'récompenser', 'par', 'ton', 'progrès', '.', 'progrès', ',', 'réel', 'source', 'de', 'espoir', '.', 'dans', 'un', 'quotidien', 'trop', 'souvent', 'anxyogèn', '.', 'mon', 'fils', ',', 'quel', 'bataille', 'pour', 'te', 'aider', 'à', 'te', 'épanouir', '.', 'et', 'te', 'permettre', 'de', 'rejoindre', 'le', 'communauté', 'humain', '.', 'autisme', ',', 'en', '2019', ',', 'en', 'France', ',']\n",
      "['autre', 'thématique', 'de', 'autre', 'sujet', 'émerger', 'au', 'sein', 'de', 'doléance', 'analyser', '.', 'si', 'le', 'thématique', 'être', 'divers', ',', 'il', 'être', 'néanmoins', 'possible', 'de', 'discerner', 'quelque', 'grand', 'ensemble', ',', 'pas', 'toujours', 'homogène', '.', 'ainsi', ',', 'le', 'accès', 'au', 'très', 'haut', 'débit', ',', 'le', 'nécessité', 'de', 'mettre', 'un', 'place', 'un', 'accompagnement', 'au', 'numérique', 'et', 'un', 'arrêt', 'de', 'le', 'suppression', 'de', 'service', 'public', 'au', 'profit', 'de', 'procédure', 'totalement', 'dématérialiser', 'revenir', 'de', 'manière', 'récurrent', '.', 'mise', 'en', 'place', 'de', 'relais', 'internet', 'pour', 'le', 'formalité', 'administratif', '(', 'sinon', ',', 'discrimination', ')', 'arrêter', 'de', 'imposer', 'de', 'passer', 'par', 'internet', 'pour', 'tout', 'le', 'service', 'public', ':', 'rien', 'ne', 'remplacer', 'le', 'contact', 'humain', 'garantir', 'le', 'accès', 'au', 'très', 'haut', 'débit', 'pour', 'tout', ',', 'généralisation', 'de', 'le', 'fibre', 'optique', 'pour', 'tout', 'le', 'territoire', 'combler', 'le', 'zone', 'blanc', 'et', 'mettre', 'en', 'place', 'le', 'haut', 'débit', 'dans', 'le', 'commune', '.', 'trouver', 'un', 'solution', 'à', 'le', 'fracture', 'numérique', 'conserver', 'le', 'accès', 'au', 'droit', 'de', 'citoyen', 'en', 'son', 'donner', 'le', 'moyen', 'financier', ',', 'de', 'accompagnement', 'et', 'de', 'accès', 'au', 'numérique', '(', 'accès', 'à', 'internet', 'à', 'un', 'prix', 'décent', ',', 'doter', 'ier', 'commune', 'de', 'moyen', 'pour', 'accompagner', 'le', 'personne', 'non', 'former', ',', 'créer', 'un', 'lieu', 'de', 'information', 'et', 'de', 'formation', 'au', 'numérique', '...', ')', 'se', 'intéresser', 'à', 'le', 'installation', 'effectif', 'de', '«', 'autoroute', '»', 'de', 'numérique', 'pour', 'le', 'territoire', 'rural', 'ainsi', 'que', 'un', 'borne', 'pour', 'le', 'téléphone', 'portable', '.', 'le', 'question', 'de', 'le', 'école', 'et', 'de', 'le', 'formation', 'en', 'général', 'revenir', 'à', 'plusieurs', 'reprise', ',', 'que', 'il', 'se', 'agir', 'de', ':', 'le', 'école', 'rural', ',', 'repenser', 'le', 'orientation', 'de', 'lycéen', ',', 'de', 'abandonner', 'le', 'réforme', 'de', 'parcours', 'Sup', \"'\", ',', 'favoriser', 'le', 'apprentissage', ',', 'donner', 'plus', 'de', 'moyen', 'à', 'le', 'enseignement', 'supérieur', '(', '«', 'ne', 'pas', 'limiter', 'le', 'accès', 'à', 'faculté', '»', ',', '«', 'ne', 'pas', 'augmenter', 'le', 'frais', 'de', 'inscription', 'de', 'étudiant', 'étranger', 'non', 'UE', '»', '...', ')', 'et', 'de', 'aider', 'davantage', 'le', 'étudiant', '(', '«', 'revoir', 'le', 'frais', 'de', 'inscription', 'et', 'de', 'sécurité', 'social', 'étudiant', 'obligatoire', ',', 'trop', 'élevé', '»', ',', '«', 'baisser', 'le', 'coût', 'de', 'étude', '»', ',', '«', 'mettre', 'en', 'place', 'un', 'crédit', 'à', 'le', 'consommation', 'pour', 'le', 'jeune', '»', '...', ')', ',', 'en', 'faire', 'un', '«', 'audit', 'de', 'besoin', '»', '.', 'favoriser', '/', 'améliorer', 'le', 'formation', 'professionnel', '.', 'amrf//', 'document', 'confidentiel', '//', 'page', '24', 'le', 'apprentissage', 'de', 'civisme', ',', 'dès', 'le', 'plus', 'jeune', 'âge', 'ou', 'plus', 'tard', ',', 'être', 'également', 'un', 'idée', 'qui', 'revenir', '(', '«', 'initier', 'au', 'civisme', ',', 'à', 'le', 'politique', ',', 'à', 'le', 'vie', 'public', ',', 'pour', 'expliquer', 'ce', 'que', 'être', 'le', 'etat', 'et', 'comment', 'se', 'financent', 'le', 'service', 'public', '»', '.', '«', 'rétablir', 'le', 'instruction', 'civique', 'à', 'le', 'école', '»', ',', 'ou', 'encore', 'intégrer', 'un', '«', 'service', 'citoyen', 'de', 'jeune', '»', 'de', 'plusieurs', 'mois', 'dans', 'son', 'cursus', 'scolaire', '.', 'le', 'besoin', 'de', 'recréer', 'de', 'lien', 'social', 'revenir', 'régulièrement', '(', 'sou', 'différent', 'angle', ':', '«', 'beaucoup', 'de', 'solitude', 'et', 'de', 'demande', 'de', 'aide', ',', 'alors', 'que', 'le', 'travail', 'et', 'le', 'association', 'permettre', 'de', 'se', 'sentir', 'utile', 'et', 'de', 'échanger', '»', '.', 'en', 'parallèle', ',', 'le', 'demande', 'de', 'davantage', 'de', 'soutien', 'à', 'le', 'vie', 'associatif', 'revenir', 'régulièrement', '(', '«', 'maintien', 'de', 'emploi', '-', 'aider', 'pour', 'le', 'commune', 'et', 'le', 'association', ',', 'ou', '«', 'développer', 'le', 'lien', 'social', 'et', 'le', 'accompagnement', 'enfin', 'de', 'vie', '»', ',', '«', 'dialogue', 'philosophique', 'à', 'le', 'échelle', 'local', '»', ')', '.', 'le', 'demande', 'de', 'un', 'réforme', 'profond', 'de', 'système', 'financier', 'et', 'monétaire', '(', '«', 'véritable', 'surveillance', 'de', 'dérive', 'de', 'banque', 'et', 'assurance', '»', ')', '.', 'le', 'demande', 'de', 'un', 'réforme', 'de', 'système', 'judiciaire', '(', '«', 'application', 'strict', 'de', 'le', 'loi', '»', ',', '«', 'rétablir', 'un', 'justice', 'efficace', '»', ',', '«', 'ramener', 'le', 'majorité', 'civil', 'à', 'seize', 'an', ',', 'pour', 'responsabiliser', 'le', 'jeune', ',', '«', 'faire', 'payer', 'un', 'forfait', 'internat', 'à', 'détenu', ',', 'comme', 'le', 'parent', 'payer', 'le', 'internat', 'de', 'enfant', '»', ')', ',', 'ou', 'de', 'un', 'plus', 'grand', 'indépendance', 'de', 'le', 'Justice', '{', '«', 'institution', 'de', 'un', 'véritable', 'indépendance', 'de', 'le', 'justice', '»', ')', '.', 'garantir', 'le', 'indépendance', 'de', 'le', 'presse', 'et', 'de', 'média', '.', 'on', 'note', 'un', 'demande', 'sur', 'le', 'sécurité', ':', '«', 'renforcer', 'le', 'moyen', 'de', 'le', 'gendarmerie', 'et', 'de', 'le', 'police', '(', 'plus', 'de', 'effectif', 'et', 'local', 'plus', 'décent', ',', '«', 'plus', 'de', 'sécurité', '(', 'police', ')', '»', ',', '«', 'ce', 'être', 'le', 'casseur', 'qui', 'devoir', 'payer', 'le', 'dégât', ',', 'non', 'le', 'peuple', 'avec', 'son', 'impôt', '»', ',', '«', 'légiférer', 'sur', 'le', 'violence', 'à', 'femme', '»', ')', '.', 'ou', 'encore', ',', '«', 'rétablissement', 'de', 'le', 'ordre', 'républicain', '(', 'retour', 'de', 'le', 'valeur', 'de', 'respect', ')', '»', ',', '«', 'rétablir', 'le', 'service', 'national', '.', 'retour', 'de', 'service', 'militaire', '»', '.', 'un', 'aspiration', 'à', 'un', 'vie', 'meilleur', ',', 'ou', 'encore', 'à', 'un', 'humanisme', 'plus', 'développer', '(', '«', 'redonner', 'au', 'peuple', 'dignité', 'et', 'respect', 'pour', 'plus', 'de', 'bonheur', '»', ',', '«', 'on', 'vouloir', 'de', 'bien-être', ',', 'et', 'pas', 'seulement', 'de', 'pouvoir', 'de', 'achat', '»', ',', '«', 'je', 'ne', 'faire', 'que', 'travailler', 'et', 'payer', 'mon', 'facture', ',', 'ce', 'ne', 'être', 'pas', 'normal', '»', ',', '«', 'votre', 'peuple', 'ne', 'réclame', 'pas', 'le', 'luxe', ',', 'mais', 'le', 'droit', 'de', 'vivre', 'correctement', '«', ',', '«', 'se', 'battre', 'pour', 'le', 'démocratie', ',', 'le', 'paix', ',', 'le', 'égalité', 'et', 'le', 'fraternité', '.', '»', ',', '«', 'respecter', 'le', 'droit', 'de', 'le', 'homme', '»', ',', '«', 'lutter', 'contre', 'le', 'racisme', '»', ',', '«', 'fin', 'de', 'le', 'société', 'de', 'consommation', ':', 'vivre', 'avec', 'le', 'confort', 'minimum', '.', '»', ',', '«', 'reconsidérer', 'le', 'humain', 'comme', 'centre', 'de', 'préoccupation', 'sociétal', '»', 'pêle-mêle', ',', 'de', 'autre', 'sujet', 'émerger', ':', '«', 'pluralité', 'de', 'campagne', 'mais', 'unicité', 'de', 'ville', '»', 'à', 'prendre', 'en', 'compte', '»', ',', '«', 'Simplifier', 'démarche', 'administratif', '»', ',', '«', 'assouplir', 'le', 'règle', 'de', 'urbanisme', 'en', 'milieu', 'rural', '»', ',', '«', 'soutenir', 'et', 'développer', 'un', 'micro-coopérative', 'proposer', 'un', 'produit', 'de', 'qualité', ',', 'local', 'en', 'circuit', 'court', '.', '»', ',', '«', 'accès', 'à', 'le', 'eau', 'potable', 'pour', 'tout', '.', '»', ',', '«', 'stopper', 'le', 'course', 'à', 'armement', 'et', 'à', 'marché', '.', '»', ',', '«', 'arrêt', 'de', 'vaccination', 'obligatoire', '»', ',', '«', 'soutenir', 'le', 'langue', 'français', ':', 'un', 'dicter', 'par', 'semaine', 'à', 'le', 'télévision', '»', ',', '«', 'supprimer', 'le', 'redevance', 'télé', '»', '...', 'amrf//', 'document', 'confidentiel', '//', 'page', '25']\n",
      "['le', '24.01.2019', '-', 'baisse', 'de', 'tva', 'par', 'le', 'produit', 'alimentaire', '-', 'salaire', 'égalité', 'pour', 'le', 'femme', '(', 'surtout', 'le', 'femme', 'seul', ')', '-', 'le', 'camion', 'servir', 'le', 'train', 'pour', 'dégager', 'le', 'route']\n",
      "Total time: 0.0010001659393310547\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "nostopword = remove_stopwords(df[\"lemmatized_text\"])\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Total time: {}\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77238325",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopword[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fa2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
