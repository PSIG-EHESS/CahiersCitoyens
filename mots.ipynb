{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ehess\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import spacy\n",
    "# import torch\n",
    "\n",
    "df = pd.read_csv(\"contrib_from_csv.csv\", sep = \",\", dtype= str)\n",
    "\n",
    "# echantillon de 10000 pour tester\n",
    "df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catégorie</th>\n",
       "      <th>Date de réception</th>\n",
       "      <th>Code postal</th>\n",
       "      <th>Code INSEE</th>\n",
       "      <th>Numéro d'ordre arbitraire</th>\n",
       "      <th>Type Graphie TT</th>\n",
       "      <th>Numéro de page</th>\n",
       "      <th>Numéro séquentiel</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>joined_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158262</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>79230</td>\n",
       "      <td>79216</td>\n",
       "      <td>11399</td>\n",
       "      <td>MD</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>Je recrute\\nPoints de satisfaction\\nAdministra...</td>\n",
       "      <td>CC_79230_190225_79216_MD_11399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61667</th>\n",
       "      <td>CC</td>\n",
       "      <td>190315</td>\n",
       "      <td>35170</td>\n",
       "      <td>35047</td>\n",
       "      <td>18231</td>\n",
       "      <td>D</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>Contact:\u000b",
       "Jane DOE\u000b",
       "Message :\u000b",
       "- Sortir de l'UE. ...</td>\n",
       "      <td>CC_35170_190315_35047_MD_18231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50281</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>53000</td>\n",
       "      <td>53130</td>\n",
       "      <td>10028</td>\n",
       "      <td>D</td>\n",
       "      <td>143</td>\n",
       "      <td>83</td>\n",
       "      <td>CONTRIBUTIONS AU GRAND DÉBAT NATIONAL\u000b",
       "QUESTION...</td>\n",
       "      <td>CC_53000_190225_53130_MD_10028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142631</th>\n",
       "      <td>CC</td>\n",
       "      <td>190227</td>\n",
       "      <td>91280</td>\n",
       "      <td>91573</td>\n",
       "      <td>13228</td>\n",
       "      <td>MD</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Fin des pages écrites</td>\n",
       "      <td>CC_91280_190227_91573_MD_13228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192341</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>29200</td>\n",
       "      <td>29019</td>\n",
       "      <td>06646</td>\n",
       "      <td>MD</td>\n",
       "      <td>241</td>\n",
       "      <td>329</td>\n",
       "      <td>retraiteretraite revenir à la défiscalisation ...</td>\n",
       "      <td>CC_29200_190225_29019_MD_06646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213594</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>29660</td>\n",
       "      <td>29023</td>\n",
       "      <td>06755</td>\n",
       "      <td>MD</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>problème de retraite siège le seuil des retrai...</td>\n",
       "      <td>CC_29660_190225_29023_MD_06755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42584</th>\n",
       "      <td>CC</td>\n",
       "      <td>190301</td>\n",
       "      <td>91200</td>\n",
       "      <td>91027</td>\n",
       "      <td>14372</td>\n",
       "      <td>MD</td>\n",
       "      <td>115</td>\n",
       "      <td>37</td>\n",
       "      <td>Nous nous proposons de recueillir vos idées et...</td>\n",
       "      <td>CC_91200_190301_91027_MD_14372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177477</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>78100</td>\n",
       "      <td>78551</td>\n",
       "      <td>02737</td>\n",
       "      <td>MD</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>protection de service case de ce système qui a...</td>\n",
       "      <td>CC_78100_190225_78551_MD_02737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176576</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>78840</td>\n",
       "      <td>78255</td>\n",
       "      <td>02679</td>\n",
       "      <td>MD</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Un très petit résumé de la situation de la Fra...</td>\n",
       "      <td>CC_78840_190225_78255_MD_02679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8880</th>\n",
       "      <td>CC</td>\n",
       "      <td>190225</td>\n",
       "      <td>45250</td>\n",
       "      <td>45053</td>\n",
       "      <td>01596</td>\n",
       "      <td>M</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>Le 09-01-2019\u000b",
       "Mr LE BAIL Lucien\u000b",
       "4 rue cru-veil...</td>\n",
       "      <td>CC_45250_190225_45053_MD_01596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Catégorie Date de réception Code postal Code INSEE   \n",
       "158262        CC            190225       79230      79216  \\\n",
       "61667         CC            190315       35170      35047   \n",
       "50281         CC            190225       53000      53130   \n",
       "142631        CC            190227       91280      91573   \n",
       "192341        CC            190225       29200      29019   \n",
       "...          ...               ...         ...        ...   \n",
       "213594        CC            190225       29660      29023   \n",
       "42584         CC            190301       91200      91027   \n",
       "177477        CC            190225       78100      78551   \n",
       "176576        CC            190225       78840      78255   \n",
       "8880          CC            190225       45250      45053   \n",
       "\n",
       "       Numéro d'ordre arbitraire Type Graphie TT Numéro de page   \n",
       "158262                     11399              MD             36  \\\n",
       "61667                      18231               D             41   \n",
       "50281                      10028               D            143   \n",
       "142631                     13228              MD              5   \n",
       "192341                     06646              MD            241   \n",
       "...                          ...             ...            ...   \n",
       "213594                     06755              MD             12   \n",
       "42584                      14372              MD            115   \n",
       "177477                     02737              MD             39   \n",
       "176576                     02679              MD              3   \n",
       "8880                       01596               M             19   \n",
       "\n",
       "       Numéro séquentiel                                       Contribution   \n",
       "158262                12  Je recrute\\nPoints de satisfaction\\nAdministra...  \\\n",
       "61667                 25  Contact:\n",
       "Jane DOE\n",
       "Message :\n",
       "- Sortir de l'UE. ...   \n",
       "50281                 83  CONTRIBUTIONS AU GRAND DÉBAT NATIONAL\n",
       "QUESTION...   \n",
       "142631                 2                              Fin des pages écrites   \n",
       "192341               329  retraiteretraite revenir à la défiscalisation ...   \n",
       "...                  ...                                                ...   \n",
       "213594                 7  problème de retraite siège le seuil des retrai...   \n",
       "42584                 37  Nous nous proposons de recueillir vos idées et...   \n",
       "177477                33  protection de service case de ce système qui a...   \n",
       "176576                 1  Un très petit résumé de la situation de la Fra...   \n",
       "8880                   9  Le 09-01-2019\n",
       "Mr LE BAIL Lucien\n",
       "4 rue cru-veil...   \n",
       "\n",
       "                             joined_id  \n",
       "158262  CC_79230_190225_79216_MD_11399  \n",
       "61667   CC_35170_190315_35047_MD_18231  \n",
       "50281   CC_53000_190225_53130_MD_10028  \n",
       "142631  CC_91280_190227_91573_MD_13228  \n",
       "192341  CC_29200_190225_29019_MD_06646  \n",
       "...                                ...  \n",
       "213594  CC_29660_190225_29023_MD_06755  \n",
       "42584   CC_91200_190301_91027_MD_14372  \n",
       "177477  CC_78100_190225_78551_MD_02737  \n",
       "176576  CC_78840_190225_78255_MD_02679  \n",
       "8880    CC_45250_190225_45053_MD_01596  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehess\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\cuda\\__init__.py:350: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  torch._C._cuda_setDevice(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  7% | 29% |\n"
     ]
    }
   ],
   "source": [
    "is_using_gpu = spacy.prefer_gpu()\n",
    "\n",
    "if is_using_gpu:\n",
    "    print(\"Using GPU!\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xob_process_breakline = lambda x: x.replace(\"\\x0b\", \" \")\n",
    "n_processbreaklines = lambda x: x.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_xob\"] = df[\"Contribution\"].apply(xob_process_breakline)\n",
    "df[\"clean_n\"] = df[\"clean_xob\"].apply(n_processbreaklines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je recrute Points de satisfaction Administration réferente Difficultés rencontrées Administration réferente'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO remove \"illisible\"\n",
    "\n",
    "documents = df.clean_n.tolist()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop) + [\"\\n\"]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = final_stopwords_list\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopword = remove_stopwords(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lemmatize(docs, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "  '''\n",
    "  Performs lemmization of input documents.\n",
    "  Args:\n",
    "    - docs: list of strings with input documents\n",
    "    - allowed_postags: list of accepted Part of Speech (POS) types\n",
    "  Output:\n",
    "    - list of strings with lemmatized input\n",
    "  '''\n",
    "  nlp = spacy.load(\"fr_core_news_sm\", disable = [\"parser\", \"ner\"])\n",
    "  nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "  lemmatized_docs = []\n",
    "  for doc in docs:\n",
    "    doc = nlp(doc)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "#       if token.pos_ in allowed_postags:\n",
    "#         tokens.append(token.lemma_)\n",
    "        tokens.append(token.lemma_)\n",
    "    lemmatized_docs.append(\" \".join(tokens))\n",
    "  return (lemmatized_docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatized = lemmatize(nostopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142722    31.12.2018 . indexation retraites niveau infla...\n",
      "27066     révolte gilets jaunes mérite réveiller conscie...\n",
      "128861    10 janvier 2019 niggemann patrick 116 rue past...\n",
      "30479     objet : propositions sujets cadre grand débat ...\n",
      "125689    grand débat national \" debat éclairer décision...\n",
      "                                ...                        \n",
      "109900                                                  NaN\n",
      "222554                                                  NaN\n",
      "93436                                                   NaN\n",
      "214835                                                  NaN\n",
      "143696                                                  NaN\n",
      "Name: cleaned, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m__init__.pxd:942\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtop2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Top2Vec\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\top2vec\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtop2vec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTop2Vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Top2Vec\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.0.29\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\top2vec\\Top2Vec.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphrases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Phrases\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\hdbscan\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhdbscan_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN, hdbscan\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobust_single_linkage_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobustSingleLinkage, robust_single_linkage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validity_index\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\hdbscan\\hdbscan_.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hdbscan_linkage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     single_linkage,\n\u001b[0;32m     22\u001b[0m     mst_linkage_core,\n\u001b[0;32m     23\u001b[0m     mst_linkage_core_vector,\n\u001b[0;32m     24\u001b[0m     label,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hdbscan_tree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     condense_tree,\n\u001b[0;32m     28\u001b[0m     compute_stability,\n\u001b[0;32m     29\u001b[0m     get_clusters,\n\u001b[0;32m     30\u001b[0m     outlier_scores,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hdbscan_reachability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mutual_reachability, sparse_mutual_reachability\n",
      "File \u001b[1;32mhdbscan\\_hdbscan_linkage.pyx:1\u001b[0m, in \u001b[0;36minit hdbscan._hdbscan_linkage\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhdbscan\\dist_metrics.pyx:13\u001b[0m, in \u001b[0;36minit hdbscan.dist_metrics\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m__init__.pxd:944\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 16:25:29,343 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-04-11 16:25:33,252 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-04-11 16:26:45,189 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-04-11 16:27:14,975 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-04-11 16:27:15,506 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['gilet', 'longtemps', 'peuple', ..., 'beaucoup', 'dormir',\n",
       "        'decennie'],\n",
       "       ['pollution', 'electrique', 'production', ..., 'agricole',\n",
       "        'installation', 'fabrication'],\n",
       "       ['senateur', 'privilege', 'ministre', ..., 'seance', 'voyage',\n",
       "        'salair'],\n",
       "       ...,\n",
       "       ['conditionnel', 'appel', 'victime', ..., 'naturel', 'justice',\n",
       "        'tenue'],\n",
       "       ['page', 'resumer', 'mail', ..., 'lundi', 'message', 'ville'],\n",
       "       ['conditionnel', 'liberation', 'penal', ..., 'agresseur', 'tenir',\n",
       "        'estime']], dtype='<U15')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df.shape\n",
    "print(rows//1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = \"\\n\"\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop)\n",
    "\n",
    "\n",
    "tokenize = lambda x : nlp(x)\n",
    "# out = random_sample.apply(tokenize)\n",
    "allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    "\n",
    "def preprocess():\n",
    "    nlp = spacy.load(\"fr_core_news_sm\", disable = [\"parser\", \"ner\"])\n",
    "    nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "    tokens = nlp(text) # Converts the text to a list of tokens (strings).\n",
    "#     tokens = [token.lower_ for token in tokens] # Lowercases all tokens.\n",
    "    lemmatized_lines = []\n",
    "    for token in tokens :\n",
    "        sentences = (sent.text)\n",
    "        lemmatized_lines.append(token.lemma_)\n",
    "\n",
    "#         lemmatized_lines.append(token.lemma if token.pos_ in allowed_postag)\n",
    "\n",
    "    out = \" \".join(lemmatized_lines)\n",
    "#     out = remove_stopwords(out)\n",
    "#     stopwords = final_stopwords_list\n",
    "#     text = [word for word in tokens if word not in stopwords] #remove the stopwords\n",
    "#     print(text)\n",
    "    return out\n",
    "\n",
    "\n",
    "# text = FILE.read()    \n",
    "# doc = nlp(text)\n",
    "# with open(\"FILEE.xml\",'w') as fp:  \n",
    "#      for sent in doc.sents:\n",
    "#          sentences = (sent.text)\n",
    "#          for match_id, start, end in phrase_matcher(nlp(sentences)):  \n",
    "#              if nlp.vocab.strings[match_id] in [\"subordoT\"]:\n",
    "#                 fp.write(\"%s\\n\" % sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Collectif des Petits Patrons et Artisans - Antenne Falaise Relais local du Collectif des Petits Patrons et Artisans, nous remercions la municipalité en générai et le Maire en particulier de prendre le temps de relayer les revendications des citoyens. Vous êtes une courroie de transmission essentielle entre les Administrés et l\\'exécutif, parfois devenu sourd à notre bon sens \"paysan\". Les Petits Patrons et Artisans sont des gens simples, travailleurs, entrepreneurs en bâtiment, chauffagistes, plombiers, peintres, électriciens, vitriers, artisans, agriculteurs, commerçants, distributeurs, transporteurs, ambulanciers, consultants, restaurateurs, petits industriels, sociétés de services informatiques, développeurs web, entreprises digitales, attachés de presse, sociétés de communication, entreprises de nettoyage, de logistique, de stockage, de déménagement, paysagistes, sociétés de jardinage... Les petites entreprises sont donc présentes partout en France, notamment à Falaise et dans ses alentours. Nos TPE sont plus de 3 500 000 en France, et c\\'est forcément par nos petites entreprises que sera résolue la question dramatique du chômage. Imaginez que chacun d\\'entre nous n\\'embauche ne serait-ce qu\\'une personne supplémentaire. À notre petite échelle, nous prenons des risques tout au long de notre carrière : risque d\\'entreprendre, de nous endetter, de stocker, d\\'embaucher, et de faire face à toutes les contraintes administratives et comptables. En janvier 2013, le Ministre Jérôme Cahuzac a instauré une Taxe insupportable, insoutenable pour les millions de gérants majoritaires de SARL que nous sommes : la soumission de nos (modestes) dividendes aux cotisations sociales I Le pire, c\\'est que cela ne concerne QUE les entrepreneurs indépendants, pas les plus riches (Sociétés anonymes, SAS...). Ainsi, après avoir payé nos charges, nos salaires, quand il nous reste un résultat, nous payons 33% d\\'impôt sur les Société, puis l\\'impôt sur le revenu (selon la tranche)... et depuis 5 ans la Taxe Cahuzac, soit 42% ! Faites le calcul ! Nous sommes ainsi moins taxés en investissant à la Bourse ou dans une \"start-up\" que dans notre propre entreprise. Le dividende rémunère un risque, au nom de quoi est-il soumis à des cotisations sociales ? Cette aberration économique étouffe littéralement les TPE, poumons économique de nos territoires. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_n\"][18927]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 22\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc :\n\u001b[0;32m     21\u001b[0m         sentences \u001b[38;5;241m=\u001b[39m (sent\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 22\u001b[0m         lemmatized_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241m.\u001b[39mlemma_)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#         lemmatized_lines.append(token.lemma if token.pos_ in allowed_postag)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_lines)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"cleaned\"] = df[\"clean_n\"][0:2].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower = lambda x : [token.lower_ for token in x] # Lowercases all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lower_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : [token\u001b[38;5;241m.\u001b[39mlower_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : [\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'lower_'"
     ]
    }
   ],
   "source": [
    "# df[\"cleaned\"] = df[\"cleaned\"].apply(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(docs, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "  '''\n",
    "  Performs lemmization of input documents.\n",
    "  Args:\n",
    "    - docs: list of strings with input documents\n",
    "    - allowed_postags: list of accepted Part of Speech (POS) types\n",
    "  Output:\n",
    "    - list of strings with lemmatized input\n",
    "  '''\n",
    "  nlp = spacy.load(\"fr_core_news_sm\", disable = [\"parser\", \"ner\"])\n",
    "  nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "  lemmatized_docs = []\n",
    "  lemmatized_lines = []\n",
    "\n",
    "#   listdocs = docs.tolist()\n",
    "#   for doc in listdocs:\n",
    "  listoflines = [nlp(line) for line in docs]\n",
    "#     docs = nlp(docs)\n",
    "  tokens = []\n",
    "  for line in listoflines :\n",
    "    listtokens = [token.lemma for token in line if token.pos_ in allowed_postag]\n",
    "#       for token in doc:\n",
    "#         if token.pos_ in allowed_postags:\n",
    "#         tokens.append(token.lemma_)\n",
    "  lemmatized_lines.append(\" \".join(listtokens))\n",
    "\n",
    "  return (lemmatized_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
