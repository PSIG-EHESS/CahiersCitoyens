{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba050edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665f6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour un ordi avec GPU :\n",
    "\n",
    "import GPUtil\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b84c850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98135ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set CUDA_PATH=C:\\Users\\ehess\\anaconda3\\Lib\\site-packages\\numba\\cuda\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83f10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_gpu()\n",
    "# spacy.prefer_gpu()\n",
    "#  j'utilise thinc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebbc49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% |  8% |\n"
     ]
    }
   ],
   "source": [
    "# pour un ordi avec GPU\n",
    "# j'utilise thinc\n",
    "\n",
    "# is_using_gpu = spacy.prefer_gpu()\n",
    "# if is_using_gpu:\n",
    "#     print(\"Using GPU!\")\n",
    "#     torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "#     print(\"GPU Usage\")\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a96769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fa0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"contrib_from_csv.csv\", sep = \",\", encoding = \"utf-8\", dtype= str)\n",
    "\n",
    "# sample d'un petit échantillon\n",
    "df = df.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa74bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les fin de lignes (deux types de codages)\n",
    "join_breaklines = lambda x : x.replace(\"\\x0b\",\" \") \n",
    "join_breaklinesn = lambda x : x.replace(\"\\n\",\" \") \n",
    "\n",
    "# supprimer les UNK des ce corpus (trois types de codages)\n",
    "kill_texte_illisible = lambda x : x.replace(\"texte illisible\", \"\")\n",
    "kill_illisible = lambda x : x.replace(\"illisible\", \"\")\n",
    "kill_illisible_plural = lambda x : x.replace(\"illisibles\", \"\")\n",
    "kill_illisible_capslock = lambda x : x.replace(\"ILLISIBLE\", \"\")\n",
    "kill_illisibleS_capslock = lambda x : x.replace(\"ILLISIBLES\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44de22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"] = df[\"Contribution\"].apply(join_breaklines)\n",
    "df[\"clean\"] = df[\"clean\"].apply(join_breaklinesn)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_texte_illisible)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible_plural)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisible_capslock)\n",
    "df[\"clean\"] = df[\"clean\"].apply(kill_illisibleS_capslock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2222b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu, use_pytorch_for_gpu_memory, set_active_gpu\n",
    "set_gpu_allocator(\"pytorch\")\n",
    "import csv   \n",
    "\n",
    "def lemmatize(doc, subset):\n",
    "    '''\n",
    "    Performs lemmization of input documents.\n",
    "    Args:\n",
    "    - docs: one column of a dataframe\n",
    "    Output:\n",
    "    - list (nb of contributions) of list (one cell,(size 2: with position input, lemmatized input)) of list of strings\n",
    "    '''\n",
    "\n",
    "    \n",
    "    set_gpu_allocator(\"pytorch\")\n",
    "    require_gpu()\n",
    "    set_active_gpu(0)\n",
    "    if prefer_gpu():\n",
    "        print(\"Using GPU!\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        use_pytorch_for_gpu_memory()\n",
    "\n",
    "        print(\"GPU Usage\")\n",
    "        GPUtil.showUtilization()\n",
    "\n",
    "    nlp = spacy.load(\"fr_core_news_md\", disable = [\"parser\", \"ner\"])\n",
    "#   modèle transformer basé sur CamemBERT. Favorise la précision au lieu de l'efficacité :\n",
    "#     nlp = spacy.load(\"fr_dep_news_trf\", disable = [\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "    spacy.require_gpu()\n",
    "\n",
    "    nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    len_data = doc.shape[0]\n",
    "    nb_batch = len_data // subset\n",
    "    size_last_batch = len_data%subset\n",
    "    first_idx = 0\n",
    "    last_idx = 0\n",
    "        \n",
    "    for batch_nb in range(nb_batch) :\n",
    "        t1 = time.time()\n",
    "        lemma_text_list = []\n",
    "        lemma_pos_list = []\n",
    "        \n",
    "        if batch_nb == nb_batch -1 :\n",
    "            last_idx += size_last_batch\n",
    "        else :\n",
    "            last_idx += subset\n",
    "        \n",
    "        batch = doc[first_idx : last_idx]\n",
    "        \n",
    "        for line in nlp.pipe(batch):\n",
    "#             ta = time.time()\n",
    "            lemma_pos_list.append([x.pos_ for x in line])\n",
    "#             GPUtil.showUtilization()\n",
    "\n",
    "            lemma_text_list.append([x.lemma_ for x in line])\n",
    "#             tc = time.time()\n",
    "#             print(f\"spacy computing time per line: {tc-ta}\")\n",
    "\n",
    "        print(\"GPU Usage\")\n",
    "        GPUtil.showUtilization()\n",
    "        \n",
    "        if batch_nb == 0 :\n",
    "            with open('lemma_temp.csv', 'w', encoding = \"utf-8\") as f :\n",
    "                writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, escapechar = '\\\\')\n",
    "                writer.writerow(lemma_text_list)\n",
    "                f.close()\n",
    "              \n",
    "            with open('pos_temp.csv', 'w', encoding = \"utf-8\") as f :\n",
    "                writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, escapechar = '\\\\')\n",
    "                writer.writerow(lemma_pos_list)\n",
    "                f.close()\n",
    "            \n",
    "        else :\n",
    "            with open(r'lemma_temp', 'a', encoding = \"utf-8\") as f:\n",
    "                writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, escapechar = '\\\\')\n",
    "                writer.writerow(lemma_text_list)\n",
    "                f.close()\n",
    "                \n",
    "            with open(r'pos_temp', 'a', encoding = \"utf-8\") as f:\n",
    "                writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, escapechar = '\\\\')\n",
    "                writer.writerow(lemma_pos_list)\n",
    "                f.close()\n",
    "        \n",
    "        del lemma_pos_list\n",
    "        del lemma_text_list\n",
    "#             tokvecs = line._.trf_data.tensors[-1] pour l'utilisation de transformers\n",
    "#     for doc in nlp.pipe(doc):\n",
    "#         buffer = []\n",
    "#         lemma_text_list.append([[\" \".join(x.pos_ for x in doc)], [\" \".join(x.lemma_ for x in doc)]])     \n",
    "#        on ne join() pas à ce moment là car la list reste nécessaire pour enlever les stopwords.\n",
    "\n",
    "        t2 = time.time()\n",
    "        print(f\"end of batch: {batch_nb}, time: {t2-t1}\")\n",
    "        \n",
    "    t3 = time.time()\n",
    "    print(\"Total time: {}\".format(t3-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb46ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    print(\"chunker\")\n",
    "    GPUtil.showUtilization()\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_chunk(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=64):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        print(\"spacy pipe \")\n",
    "        GPUtil.showUtilization()\n",
    "\n",
    "    return preproc_pipe\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    print(1)\n",
    "    do = delayed(process_chunk)\n",
    "    print(2)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, texts.shape[0], chunksize=chunksize))\n",
    "    print(3)\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203211fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "chunker\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% |  8% |\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['preproc_parallel'] = preprocess_parallel(df['clean'], chunksize=1024)\n",
    "\n",
    "# AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5834a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  7% | 15% |\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 33% |\n",
      "end of batch: 0, time: 4.81217622756958\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 51% |\n",
      "end of batch: 1, time: 3.842679023742676\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 52% |\n",
      "end of batch: 2, time: 4.077171564102173\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 62% |\n",
      "end of batch: 3, time: 5.729888916015625\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 62% |\n",
      "end of batch: 4, time: 5.670403957366943\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 5, time: 7.172628879547119\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 6, time: 7.282472372055054\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 7, time: 8.261136531829834\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 8, time: 9.533139705657959\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 9, time: 9.963354587554932\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 10, time: 10.915212392807007\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 11, time: 11.382760047912598\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "end of batch: 12, time: 12.514436483383179\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 13, time: 13.8270742893219\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 14, time: 14.467057466506958\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 15, time: 15.891485929489136\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 16, time: 15.735836267471313\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 70% |\n",
      "end of batch: 17, time: 18.148460149765015\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 71% |\n",
      "end of batch: 18, time: 20.260523319244385\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 19, time: 19.36659812927246\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 70% |\n",
      "end of batch: 20, time: 21.017781496047974\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 76% |\n",
      "end of batch: 21, time: 22.331044673919678\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 76% |\n",
      "end of batch: 22, time: 22.12620186805725\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 76% |\n",
      "end of batch: 23, time: 24.555356979370117\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 75% |\n",
      "end of batch: 24, time: 24.181344747543335\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 76% |\n",
      "end of batch: 25, time: 25.078567504882812\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 76% |\n",
      "end of batch: 26, time: 26.492108583450317\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 75% |\n",
      "end of batch: 27, time: 26.863009214401245\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 75% |\n",
      "end of batch: 28, time: 28.427776336669922\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 29, time: 29.60213017463684\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 80% |\n",
      "end of batch: 30, time: 30.81364393234253\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 31, time: 33.39684796333313\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 32, time: 32.37916946411133\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 33, time: 33.49897289276123\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 34, time: 34.35961699485779\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 35, time: 35.99330496788025\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 36, time: 37.44076728820801\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 37, time: 37.9558527469635\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 38, time: 39.80496644973755\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 80% |\n",
      "end of batch: 39, time: 41.51984977722168\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 40, time: 41.08188843727112\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 41, time: 40.986063718795776\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 42, time: 45.12924790382385\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 82% |\n",
      "end of batch: 43, time: 47.81289267539978\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 44, time: 48.602152585983276\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 45, time: 46.25750541687012\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 81% |\n",
      "end of batch: 46, time: 50.25755548477173\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 47, time: 49.29868984222412\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 48, time: 48.539523124694824\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 49, time: 49.73955678939819\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 50, time: 49.929556131362915\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 80% |\n",
      "end of batch: 51, time: 52.064074754714966\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 52, time: 53.27687907218933\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 53, time: 53.94948077201843\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 54, time: 55.577516078948975\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 55, time: 56.60673117637634\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 56, time: 57.24439263343811\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 57, time: 59.48338079452515\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 58, time: 58.954832553863525\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 59, time: 61.433008909225464\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 60, time: 62.22263550758362\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 61, time: 62.78505611419678\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 62, time: 67.11799573898315\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 63, time: 65.92209053039551\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 64, time: 67.92422723770142\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 65, time: 72.13024711608887\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 66, time: 71.30100202560425\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 79% |\n",
      "end of batch: 67, time: 71.75871777534485\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 80% |\n",
      "end of batch: 68, time: 73.03299307823181\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 80% |\n",
      "end of batch: 69, time: 74.9247772693634\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 80% |\n",
      "end of batch: 70, time: 76.91193127632141\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 80% |\n",
      "end of batch: 71, time: 76.09986639022827\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 80% |\n",
      "end of batch: 72, time: 79.73770546913147\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 85% |\n",
      "end of batch: 73, time: 79.86999464035034\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 85% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of batch: 74, time: 82.20615196228027\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 85% |\n",
      "end of batch: 75, time: 84.92238473892212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# GPU Total time: 130.2461621761322 (without if and with fr_core_news_md)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 55\u001b[0m, in \u001b[0;36mlemmatize\u001b[1;34m(doc, subset)\u001b[0m\n\u001b[0;32m     51\u001b[0m             last_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m subset\n\u001b[0;32m     53\u001b[0m         batch \u001b[38;5;241m=\u001b[39m doc[first_idx : last_idx]\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe(batch):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#             ta = time.time()\u001b[39;00m\n\u001b[0;32m     57\u001b[0m             lemma_pos_list\u001b[38;5;241m.\u001b[39mappend([x\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m line])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#             GPUtil.showUtilization()\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\language.py:1574\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[0;32m   1573\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[1;32m-> 1574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\util.py:1670\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[0;32m   1661\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1662\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   1668\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1670\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1672\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\util.py:1670\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[0;32m   1661\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1662\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   1668\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1670\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1672\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:55\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:143\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    141\u001b[0m error_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations(doc, matches)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:152\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    150\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher(doc, allow_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_spans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m matches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    153\u001b[0m     (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    154\u001b[0m ]\n\u001b[0;32m    155\u001b[0m matches\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my4env\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher(doc, allow_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_spans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m matches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    153\u001b[0m     (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    154\u001b[0m ]\n\u001b[0;32m    155\u001b[0m matches\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GPU Total time: 130.2461621761322 (without if and with fr_core_news_md)\n",
    "lemmatize(df[\"clean\"], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text\"] = lemmatized_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1293d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_pos\"] = lemmatized_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787836b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour afficher l'ensemble des cells, à n'utiliser que pour afficher une portion seulement\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lemmatized_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaef954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
