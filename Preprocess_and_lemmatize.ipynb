{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available?  True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# pour la gestion du GPU, j'utilise thinc.api finalement au lieu de spacy\n",
    "\n",
    "# pour un ordi avec GPU :\n",
    "\n",
    "import GPUtil\n",
    "import torch\n",
    "!set CUDA_PATH=\"/usr/local/cuda-12\"\n",
    "GPUtil.getAvailable()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"is cuda available? \", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98135ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb30e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(df):\n",
    "    pd.set_option('display.max_colwidth', 2000)\n",
    "    # pd.set_option('display.width', 2000)\n",
    "\n",
    "    print(df)\n",
    "    # pd.reset_option('display.width')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', 2000)\n",
    "# pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# print(x)\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.float_format')\n",
    "# pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a66641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vérifier l'état des données\n",
    "\n",
    "def check_data(df_to_check):\n",
    "\n",
    "    if df_to_check.isnull().values.any() :\n",
    "        print(\"y a t il des nan? ->\",df_to_check.isnull().values.any())\n",
    "        print(\"combien y a t il de nan? ->\",df_to_check.isnull().values.sum())\n",
    "        print(\"où sont les null? ->\\n\",df_to_check.isnull().sum())\n",
    "        print(df_to_check[df_to_check.isnull().T.any()])\n",
    "    else :\n",
    "        print(\"il n'y a pas de NaN\")\n",
    "\n",
    "    print(\"quels types pour chaque colonne?\\n\")    \n",
    "    for column in df_to_check.columns:\n",
    "        print(pd.api.types.infer_dtype(df_to_check[column]))\n",
    "    print(\"y a-t-il des types mixés dans les données?\\n\")\n",
    "    print(df_to_check[column][df_to_check[column].apply(lambda x: isinstance(x, type))])\n",
    "    print(df_to_check._is_mixed_type)\n",
    "    print(df_to_check.dtypes.nunique()>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e04b2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 21% |  6% |\n"
     ]
    }
   ],
   "source": [
    "from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu, use_pytorch_for_gpu_memory, set_active_gpu\n",
    "set_gpu_allocator(\"pytorch\")\n",
    "require_gpu() \n",
    "set_active_gpu(0)\n",
    "if prefer_gpu():\n",
    "    print(\"Using GPU!\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    use_pytorch_for_gpu_memory()\n",
    "\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8838bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if spacy is using GPU\n",
    "\n",
    "gpu_usage = spacy.prefer_gpu()\n",
    "if gpu_usage:\n",
    "    print(\"spaCy is using GPU.\")\n",
    "else:\n",
    "    print(\"spaCy is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cf5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_CC_cleaned.csv\", sep = \",\", encoding = \"utf-8\", dtype= str)\n",
    "check_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4963a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is using GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lemmatize(doc, batch_size, stopwords = False):\n",
    "    \n",
    "    '''\n",
    "    - split the dataframe into batches\n",
    "    - take each batche and apply choosed nlp model from spacy\n",
    "    - retrieve lemmes texts and corresponding postags\n",
    "    \n",
    "    Input : doc = a text column from a dataframe, batch_size = an int, stopwords = boolean \n",
    "    stopwords is designed to choose if we want to keep only NOUN, ADJ, ADV, VERB lemmes and delete other\n",
    "\n",
    "    Output :  one df with lemmas texts and one df with correpsonding postags \n",
    "    '''\n",
    "\n",
    "    nlp = spacy.load(\"fr_core_news_md\", disable = [\"parser\", \"ner\"])\n",
    "    nlp.max_length = 3000000 #this is possible as we're not using parser or ner.\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # count the number of batches\n",
    "    len_data = doc.shape[0]\n",
    "    print(\"batch_size : \", batch_size)\n",
    "    nb_batch = int(len_data/batch_size)+ (len_data % batch_size > 0)\n",
    "    print(\"nb_batch : \", nb_batch)\n",
    "    # size of last bacth\n",
    "    size_last_batch = len_data%batch_size\n",
    "    print(\"size_last_batch : \", size_last_batch)\n",
    "    first_idx = 0\n",
    "    last_idx = 0\n",
    "        \n",
    "    final_lemmas_df = pd.DataFrame()\n",
    "    final_postag_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # creating one batch (a slice of the df)\n",
    "    for batch_nb in range(nb_batch) :\n",
    "        t1 = time.time()\n",
    "        lemma_text_list = []\n",
    "        lemma_pos_list = []\n",
    "        print(f\"starting batch n° {batch_nb} , on {nb_batch} batches\")\n",
    "        # range() commence à 0 et exclut nb_batch : donc on utilise nb_batch -1 pout détecter le dernier batch\n",
    "        # si c'est le dernier batch, on ajoute la taille du batch final au lieu de la taille prévue\n",
    "        if batch_nb == nb_batch -1 :\n",
    "            first_idx = last_idx\n",
    "            last_idx += size_last_batch\n",
    "        else :\n",
    "            first_idx = last_idx\n",
    "            last_idx += batch_size\n",
    "        print(f\"first_index : {first_idx} and last_idx : {last_idx}\")\n",
    "        batch = doc[first_idx : last_idx]\n",
    "        \n",
    "        # spacy tools go through the batch\n",
    "        for line in nlp.pipe(batch):\n",
    "            if stopwords is True:\n",
    "                lemma_pos_list.append([x.pos_ for x in line if x.pos_ in {\"VERB\", \"ADJ\", \"ADV\", \"NOUN\"}])\n",
    "                lemma_text_list.append([x.lemma_ for x in line if x.pos_ in {\"VERB\", \"ADJ\", \"ADV\",\"NOUN\"}])\n",
    "\n",
    "            else :\n",
    "                lemma_pos_list.append([x.pos_ for x in line])\n",
    "                lemma_text_list.append([x.lemma_ for x in line])\n",
    "\n",
    "        print(\"GPU Usage\")\n",
    "        GPUtil.showUtilization()\n",
    "\n",
    "        # writing the output lemmas and postag in one file each\n",
    "        # n contributions per file, n the batch_size\n",
    "\n",
    "        df_lemma = pd.DataFrame([lemma_text_list])\n",
    "        df_postag = pd.DataFrame([lemma_pos_list])\n",
    "\n",
    "\n",
    "        final_lemmas_df = pd.concat([final_lemmas_df,df_lemma.T])\n",
    "        final_postag_df = pd.concat([final_postag_df,df_postag.T])\n",
    "\n",
    "        # deleting the lists for clearing, to force python garbage collector to *actually* collect\n",
    "        del df_lemma\n",
    "        del df_postag\n",
    "\n",
    "        del lemma_text_list\n",
    "        del lemma_pos_list\n",
    "    \n",
    "        t2 = time.time()\n",
    "        print(f\"end of batch n°: {batch_nb}, time: {t2-t1}\")\n",
    "        \n",
    "    t3 = time.time()\n",
    "    print(\"Total time: {}\".format(t3-t0))\n",
    "    return(final_lemmas_df, final_postag_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and postag on clean text\n",
    "final_lemmas_df, final_postag_df = lemmatize(df[\"clean\"], batch_size= 256, stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15eef894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the df\n",
    "# renumber the index column (not accurate bc origin from each concatenation)\n",
    "\n",
    "forconcat_lemmas_df = final_lemmas_df.reset_index()\n",
    "forconcat_postag_df = final_postag_df.reset_index()\n",
    "\n",
    "# drop the secoond index column\n",
    "\n",
    "forconcat_lemmas_df = forconcat_lemmas_df.drop(labels = \"index\", axis=1)\n",
    "forconcat_postag_df = forconcat_postag_df.drop(labels = \"index\", axis=1)\n",
    "\n",
    "# name the columns \n",
    "\n",
    "forconcat_lemmas_df.rename(columns = {0:\"lemmas\"}, inplace=True)\n",
    "forconcat_postag_df.rename(columns = {0:\"postags\"}, inplace=True)\n",
    "\n",
    "# concatenate the columns to the side of the original csv\n",
    "\n",
    "df = pd.concat([df,forconcat_lemmas_df], axis = 1)\n",
    "df = pd.concat([df,forconcat_postag_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporter le dataframe dans un csv\n",
    "\n",
    "df.to_csv(\"df_CC_lemmas_postag.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f812c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and postagging on only verb, noun, adj, adv\n",
    "model_lemmas_df, model_postag_df = lemmatize(df[\"clean\"], batch_size= 256, stopwords = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e037481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renumber the index column (not accurate bc origin from each concatenation)\n",
    "\n",
    "stopwords_lemmas_df = model_lemmas_df.reset_index()\n",
    "stopwords_postag_df = model_postag_df.reset_index()\n",
    "\n",
    "# drop the secoond index column\n",
    "\n",
    "stopwords_lemmas_df = stopwords_lemmas_df.drop(labels = \"index\", axis=1)\n",
    "stopwords_postag_df = stopwords_postag_df.drop(labels = \"index\", axis=1)\n",
    "\n",
    "# add these stopwords_lemmas_df and  stopwords_postag_df to the global df\n",
    "df[\"lemmas_only_VERB_ADJ_ADV_NOUN\"] = stopwords_lemmas_df\n",
    "df[\"postags_only_VERB_ADJ_ADV_NOUN\"] = stopwords_postag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e831b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporter le dataframe dans un csv\n",
    "\n",
    "# df.to_csv(\"df_CC_lemmas_postag.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d48248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_CC_lemmas_postag.csv\", sep = \",\", encoding = \"utf-8\", dtype= str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc564d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il n'y a pas de NaN\n",
      "quels types pour chaque colonne?\n",
      "\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "y a-t-il des types mixés dans les données?\n",
      "\n",
      "Series([], Name: postags_only_VERB_ADJ_ADV_NOUN, dtype: object)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "check_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the stopwords, using the spacy lexicon\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop) + [\"\\n\"]\n",
    "final_stopwords_set = set(final_stopwords_list)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    output = []\n",
    "\n",
    "    for sentence_list in doc :\n",
    "        sentence_output = []\n",
    "        print('in remove sentence : ', sentence_list)\n",
    "        for word in sentence_list :\n",
    "            print(\"in remove   \", word)\n",
    "        if word not in final_stopwords_set :\n",
    "            sentence_output.append(word)\n",
    "        output.append(sentence_output)\n",
    "        \n",
    "    return(output)\n",
    "\n",
    "\n",
    "def batching_stopwords(doc_as_list, batch_size) : \n",
    "    \"\"\"\n",
    "    applique remove_dots_and_spaces() sur chaque cellule de texte du dataframe\n",
    "\n",
    "    Input : une colonne de textes sélectionnée dans un dataframe , une taille de batch\n",
    "    Output : un dataframe avec toutes les string modifiées \n",
    "    \"\"\"\n",
    "\n",
    "    # count the number of batches\n",
    "    len_data = len(doc_as_list)\n",
    "    batch_total = int(len_data/batch_size)+ (len_data % batch_size > 0)\n",
    "\n",
    "    # size of last bacth\n",
    "    size_last_batch = len_data%batch_size\n",
    "    first_idx = 0\n",
    "    last_idx = 0\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    for batch in range(batch_total) :\n",
    "\n",
    "        #  detect last batch\n",
    "        if batch == batch_total -1 :\n",
    "            first_idx = last_idx\n",
    "            last_idx += size_last_batch\n",
    "        # \n",
    "        else :\n",
    "            first_idx = last_idx\n",
    "            last_idx += batch_size\n",
    "        print(first_idx, last_idx)\n",
    "        for text_cell in doc_as_list[first_idx:last_idx] :\n",
    "            print(\"text_cell :\", text_cell)\n",
    "            output = remove_stopwords(text_cell)\n",
    "            print(\"text_cell_clean :\", text_cell)\n",
    "\n",
    "        # concatene this batch list of text with the complete dataframe \n",
    "        output_df = pd.concat([output_df, pd.DataFrame({\"name\" : output})], ignore_index= True )\n",
    "        del output\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "# doc = df.lemmas.tolist()\n",
    "# all_lemmas = batching_stopwords(doc, 256)\n",
    "# df[\"all_lemmas_without_stopwords_set\"] = all_lemmas\n",
    "\n",
    "# doc = df.lemmas_only_VERB_ADJ_ADV_NOUN.tolist()\n",
    "# some_lemmas = batching_stopwords(doc, 256)\n",
    "# df[\"VERB_ADJ_ADV_NOUN_without_stopwords\"] = some_lemmas\n",
    "# doc = df.clean.tolist()\n",
    "# some_clean_text = batching_stopwords(doc, 256)\n",
    "# df[\"clean_text_without_stopwords\"] = some_clean_text\n",
    "df_test = df[0:1000]\n",
    "doc = df_test.clean.tolist()\n",
    "some_clean_text = remove_stopwords(doc)\n",
    "df_test[\"clean_text_without_stopwords\"] = some_clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1293d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporter le dataframe dans un csv\n",
    "\n",
    "# df.to_csv(\"df_CC_lemmas_postag.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5d70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
