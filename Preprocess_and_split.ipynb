{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% |  2% |\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# pour un ordi avec GPU :\n",
    "\n",
    "import GPUtil\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "!set CUDA_PATH=\"/usr/local/cuda-12\"\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3012c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérifier l'état des données\n",
    "\n",
    "def check_data(df_to_check):\n",
    "\n",
    "    if df_to_check.isnull().values.any() :\n",
    "        print(\"y a t il des nan? ->\",df_to_check.isnull().values.any())\n",
    "        print(\"combien y a t il de nan? ->\",df_to_check.isnull().values.sum())\n",
    "        print(\"où sont les null? ->\\n\",df_to_check.isnull().sum())\n",
    "        print(df_to_check[df_to_check.isnull().T.any()])\n",
    "    else :\n",
    "        print(\"il n'y a pas de NaN\")\n",
    "\n",
    "    print(\"quels types pour chaque colonne?\\n\")    \n",
    "    for column in df_to_check.columns:\n",
    "        print(pd.api.types.infer_dtype(df_to_check[column]))\n",
    "    print(\"y a-t-il des types mixés dans les données?\\n\")\n",
    "    print(df_to_check[column][df_to_check[column].apply(lambda x: isinstance(x, type))])\n",
    "    print(df_to_check._is_mixed_type)\n",
    "    print(df_to_check.dtypes.nunique()>1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fa0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_CC_lemmas_postag.csv\", sep = \",\", encoding = \"utf-8\", dtype= str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee97be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  j'enlève les colonnes que je n'utiliserai pas avec BERTopic, pour réduire le poids du fichier \n",
    "df = df.drop([\"Contribution\",\"lemmas\", \"postags\", \"lemmas_only_VERB_ADJ_ADV_NOUN\", \"postags_only_VERB_ADJ_ADV_NOUN\", \"lemmas_no_stopwords\", \"clean_no_stopwords\", \"VERB_ADJ_ADV_NOUN_no_stopwords\"], axis=1)\n",
    "# df = df.drop([\"Contribution\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcc64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  certaines lignes n'ont que des NaN pour les stopwords, c'est parce que la contribution était seulement composée de illisibles et de stopwords\n",
    "df = df.dropna()\n",
    "\n",
    "# on élimine les rang qui ont des nan, \n",
    "# en conservant l'index originel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfdea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si on souhaite print des contributions dans toute leur longueur (ne pas appliquer sur tout le corpus)\n",
    "\n",
    "def print_full(df):\n",
    "    pd.set_option('display.max_colwidth', 3000)\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    # pd.set_option('display.max_columns', None)\n",
    "    # pd.set_option('display.width', 2000)\n",
    "    # pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    # pd.set_option('display.max_colwidth', None)\n",
    "    print(df)\n",
    "    # pd.reset_option('display.max_rows')\n",
    "    # pd.reset_option('display.max_columns')\n",
    "    # pd.reset_option('display.width')\n",
    "    # pd.reset_option('display.float_format')\n",
    "    # pd.reset_option('display.max_colwidth')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# autres options :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b3573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns without spaces (header names, not my best idea)\n",
    "\n",
    "df = df.rename(columns={\"Date de réception\" : \"Date_de_reception\", \"Code postal\" : \"Code_postal\", \"Code INSEE\" : \"Code_INSEE\",\n",
    "           \"Numéro d'ordre arbitraire\" : \"Numero_d_ordre_arbitraire\", \"Type Graphie TT\" : \"Type_Graphie_TT\" , \"Numéro de page\" : \"Numero_de_page\",\n",
    "           \"Numéro séquentiel\" : \"Numero_sequentiel\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5cc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\", disable = [\"ner\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa74bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = en commentaire, l'ensemble de règles sentencize(), sinon le modèle sents\n",
    "\n",
    "def sentencize(document, subset, column_name):\n",
    "    if subset :\n",
    "        document = document[0:subset]\n",
    "\n",
    "    t0 = time.time()\n",
    "    all_sentences_df = pd.DataFrame()\n",
    "\n",
    "    for doc, infos_doc in zip(nlp.pipe(document[column_name], batch_size=128), document.itertuples()):\n",
    "        sentence_list = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        sentence_df = pd.DataFrame({\"phrases\" : sentence_list})\n",
    "\n",
    "        metadata = pd.DataFrame([infos_doc]*len(sentence_list))\n",
    "\n",
    "        sentence_df = pd.concat([sentence_df,metadata], axis=1)\n",
    "\n",
    "        all_sentences_df = pd.concat([all_sentences_df,sentence_df])\n",
    "\n",
    "        # deleting the lists for clearing, to force python garbage collector to *actually* collect\n",
    "\n",
    "        del sentence_df\n",
    "        del sentence_list\n",
    "        del metadata\n",
    "\n",
    "            \n",
    "    t3 = time.time()\n",
    "    print(\"Total time: {}\".format(t3-t0))\n",
    "    #  ? all_sentences_df = all_sentences_df.drop(labels = \"Contribution\", axis=1)\n",
    "    return(all_sentences_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5366750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 13373.065524339676\n"
     ]
    }
   ],
   "source": [
    "# this function ASKS 10h to be run on a large dataset\n",
    "# DO NOT FORGET TO CHOOSE A SUBSET size IF TESTING\n",
    "\n",
    "sentences = sentencize(df[[\"Catégorie\",\"Date_de_reception\",\"Code_postal\",\"Code_INSEE\",\"Numero_d_ordre_arbitraire\",\n",
    "                          \"Type_Graphie_TT\",\"Numero_de_page\",\"Numero_sequentiel\",\"clean\"]],subset = None, column_name = \"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bab4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the df\n",
    "# renumber the index column (not accurate bc origin from each concatenation)\n",
    "\n",
    "sentence_df = sentences.reset_index()\n",
    "\n",
    "# drop the secoond index column\n",
    "\n",
    "sentence_df = sentence_df.drop(labels = \"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61c52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of sentence_df at the end : (2401714, 11)\n",
    "\n",
    "# turn to pickle\n",
    "sentence_df.to_pickle(\"pickle_sentences_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e884739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first version was saved as csv\n",
    "# default way should be to turn to saved to pickle since the beginnning\n",
    "\n",
    "# here I dowloaded csv format for sentences then turned it to pickle for using after\n",
    "\n",
    "# df = pd.read_csv(\"sentences_df.csv\", sep = \",\", encoding = \"utf-8\", dtype= str)\n",
    "# df.to_pickle(\"pickle_sentences_sentences.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01db2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
